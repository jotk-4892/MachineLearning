---
title: 'Problem Set1: Linear Methods'
author: "***, ***, ***"
date: "24 11 2019"

---

#=====================================================================================================================
##Task1: Multiple linear regression

#1.1

Equation: Log(FEV) = b0 + b1*(AGE)+ b2*(HTCM) + b3*(GENDER) + b4*(SMOKE)

```{r}
library(GLMsData)
data("lungcap")
lungcap$Htcm <- lungcap$Ht*2.54
lung_model <- lm(log(FEV) ~ Age + Htcm + Gender + Smoke, data=lungcap)
summary(lung_model)

```

#1.2a Log (FEV)- Plot
```{r}
plot(lungcap$Htcm, log(lungcap$FEV), main = "log(lung capacity)")
```

#1.2b FEV-Plot
```{r}
plot(lungcap$Htcm, lungcap$FEV, main = "lung capacity")
```

Answer: By plotting both normalized and logarithmic dependent variable of FEV it is clear log(FEV) shows off a clearer linear relationship than the normalized form of FEV. Through transforming the model into a log-linear model the interpretation of the coefficients changes. A 1cm increase in height increases the average lung capacity by beta percent. The normalized model shows an exponential relationship which is linearized by taking the log of the dependent variable.

#1.3 

#1.3a:Estimator
continuous: Age -> Estimate of beta1(Age) = 0,023387
-> Holding other factors constant, a one year increase in the age of an average person increases lung capacity by 2,3387 percent 
(since log-linear model, an one unit increase in x increases y by beta*100 percent)
dummy variable: Gender -> Estimate of beta4(GenderM) = 0,029319
-> Holding other factors constant, male have on average, compared to female, a 2,9319 percent higher lung capacity

#1.3b: std.Error
The standard error describes measures the average value that the estimates vary from the actual values. The lower the std. Error the better.
The std. errors are especially needed to compute Confidence Intervals and to test hypothesis for the existance of a realtionship between variables.
Continous: Age -> Std. error b1(Age) = 0.003348
-> the estimate of the standard deviation is 0.003348 here  
Dummy: Gender -> std. error b4 (GenderM) = 0.011719
-> The interpretation is normally not different if the variable is a dummy variable.

#1.3c: Residual standard error
the residual standard error is a measure of the "quality" of the regression fit. It describes the average amount that the response deviates from the true regression line.So its the difference in standard error between the observed and the predicted values. THus it is a goodness-of-fit measure to see how good the data points fit to the actual model.

#1.3d: F-Statistic
The f-Statistic is an indicator how the relationship between the predictor and the response variables in the regression model is.
The F-Statistic depends on the number of observations and the number of predictors.Here the F-statistic is 694.6 and thus for the size of the data
it is significantly different to one and thus a good F-statistic indicator. 
In R the F-statistic shows the result of an so called overall-F-Test, thus the higher the F-statistic the higher the probablity the reject the null-hypothesis that there is no realtionship between the predictors.

#1.4 
Proportion of variability equals the R-Squared of the model. R-Squared is a measurement for the fit of  the model, which takes values from 0 to 1. R-Squared of the fitted lung model equals 0.8106, which means that about  81 % of the variance can be explained by the model. Since 0.811 is close to 1, one can say the model fits the data pretty well.

#1.5
```{r}
# Best Guess:

#log(FEV ) = -1,943998 + 0,023387 x AGE + 0,016849 x Htcm + 0,029319 x GenderM - 0, 046067 x Smoke
# plug in Values for the given subject leads to:

depvar = -1.943998 + 0.023387 * 14 + 0.016849 * 175 + 0.029319 * 1  -  0.046067 * 0
exp(depvar) # the lung capacity of an average 14 year old non smoking man with height of 175 cm equals 3.901316 litres

```

```{r}
# 95%-Prediction Interval
pred.mod <- lm(log(FEV) ~ Age + Htcm+ Gender + Smoke, data=lungcap)

#Then we create a new data frame that set the waiting time value.
newdata = data.frame(Age=14, Htcm = 175, Smoke = 0, Gender = "M", data = lungcap)

#We now apply the predict function and set the predictor variable in the newdata argument. We also set the interval type as "predict", 
#and use the default 0.95 confidence level.
 exp(predict(pred.mod, newdata, interval="predict") )  # inverse exponential values of dependent variable
 # lower 95% confidence intervall equals: 2.928889
 # higher 95% confidence intervall equals: 5.196153
 
 min((lungcap$FEV)) # equals 1.75665
 max((lungcap$FEV)) # equals 5.793

``` 
Answer: Since the total range of lung capacity is reaches from  0.791 to 5.793, the lower limit does make sense. The upper limit of 5.196153 almost reaches the total limit of 5.793 and therefore it is not very expressive.A smaller conf. interval of, e.g. 99% would be more meaningful.

#1.6
```{r}
lung_model2 <-  lm(log(FEV) ~ Age + Htcm + Gender + Smoke + Smoke*Age ,data=lungcap)
summary(lung_model2)
``` 
Meaning of the estimate: interaction term used as another variable. Since it is an interaction between a dummy and a continuous regressor the interpretation is as follows: Every other regressor constant, being a smoker and gaining one more year of age reduces the lung capacity by 0.0116659 percent. comparison to AGE shows: AGE is positive related to lung capacity. This might be due to the dataset. If newborns and children are included,their lung capacity increases until a certain age. That might be the reason why AGE has a positive effect on lung capacity. If AGE interacts with smokers, newborns and children are "hopefully" not included in the dataset. But from a certain age on, full lung capacity is reached and stagnates, which is shown in the next plot.
```{r}
plot(lungcap$Age, lungcap$FEV)
```

Interaction term stat. significant: since its p-value is larger than 0,01 the interaction term is not statistically significant.

What is the effect of the inclusion of the interaction term on the statistical significance of Smoke and Age?
```{r}
summary(lung_model)
summary(lung_model2)
```
Answer: Both p-values become larger. AGE is still highly statistically significant. Smoking, which was  significant at 0.01 level gets insignificant. This is due to the fact that smoking is a Dummy variable and being a smoker is already captured in the interaction  term, since each individuum in the study has a certain age. It would have been different if the interaction term had, for example, captured smoking and being a male. In this case smoking and smoking*male would have captured different observations.

#=====================================================================================================================
##Task 2: Classification (Logit/LDA/KNN)

Given
```{r}
tennisdata <- read.csv("tennisdata.csv")

library(class)
library(caret)
library(ggplot2)
t_raw = tennisdata

#Summarized quality of the two players:

tn = na.omit(data.frame(y=as.factor(t_raw$Result),
  x1=t_raw$ACE.1-t_raw$UFE.1-t_raw$DBF.1,
  x2=t_raw$ACE.2-t_raw$UFE.2-t_raw$DBF.2))

#Repreduce Results, i.e. produce the same sample again
set.seed(4268)
#tr stands for training
#trte test data
tr = sample.int(nrow(tn),nrow(tn)/2)
trte=rep(1,nrow(tn))
trte[tr]=0

tennis=data.frame(tn,"istest"=as.factor(trte))
#View(tennis)

#Scatterplot training observations and test observations
ggplot(data=tennis,aes(x=x1,y=x2,colour=y,group=istest,shape=istest))+
geom_point()+theme_minimal()

```



# 2.1 Overview of the Data
```{r}
# Tennis is a dataset with 787 observations of 4 variables.
# y indicates who won the match, x1 and x2 are quality indices.
class(tn)
colnames(tn)
head(tn)
summary(tn)
nrow(tn)


#Median Values: x1: -24.00 & x2: -24.00
median(tn$x1)
median(tn$x2)

#Pairwise Scatterplot:

pairs(tn,col=tn$y,main ="Pairwise Scatterplots")

#Comment: Looking at the pairwise scatterplots, y seems to be positively correlated with x1 and negatively with x2. That suggests both players' qualities influencing the outcome in a for them favourable way. Since y is a factor with two levels, it is shown with values 1 and 2 instead of 0 and 1. (This could be fixed like in this corrplot:)
library(corrplot)
tn2 <- data.frame(as.numeric(tn$y)-1,tn$x1,tn$x2)
correlations <- cor(tn2)
corrplot(correlations, method="circle")
# simplified corrplot shows same direction of correlation


```



# 2.2 Logistic Regression
```{r}
# perform logistic regression and print results
glm.fit <- glm(y~x1+x2,family=binomial(link='logit'),data=tn)
summary(glm.fit)
glm.probs <- predict(glm.fit,type="response")
print(glm.probs)
#The estimated coefficients for x1 and x2 are highly statistically significant because of their p-values <=0.001

newm1 = data.frame(x1= -25, x2 = -20)
predict(glm.fit, newm1,type="response")

#actual result is y=0, meaning player 2 won. The model predicted a probability of 38.29% of player 1 winning.
tn$y[1]
glm.probs[1]

```

#2.3 Confusion Matrix
```{r}
#Confusion matrix: diagonal elements indicate correct predictions
#Sensitivity: The sensitivity is defined as the proportion of positive results out of the number of samples which were actually positive

threshold=0.5
predicted_values<-ifelse(glm.probs>threshold,1,0)
actual_values<-tn$y
conf_matrix<-table(predicted_values,actual_values)
conf_matrix

library(caret)
sensitivity(conf_matrix) 
301/(301+91) #0.7678571
specificity(conf_matrix) 
287/(108+287) #0.7265823

(301+287)/787
# 74.71% correctly predicted


#The null rate results from simply classifying each observation to the dominant class overall, which is in this case #the normal class.


```

# 2.4 Training Data set
```{r}
# divide tennis dataset into training and test datasets
train.tennis <- subset(tennis, tennis$istest==1)
# oder zB tennis[tennis$istest == 1,]
train.tennis
test.tennis <- subset(tennis, tennis$istest==0)
test.tennis

# perform logistic regression with training data

glm.fit.train <- glm(y~x1+x2,family=binomial(link='logit'),data=train.tennis)
summary(glm.fit.train)

# predict y in test data
glm.probs.test <- predict(glm.fit.train,test.tennis,type="response")
print(glm.probs.test)

#compute Confusion Matrix for test data set
threshold=0.5
predicted_values2<-ifelse(glm.probs.test>threshold,1,0)
actual_values2<-test.tennis$y
conf_matrix2<-table(predicted_values2,actual_values2)
conf_matrix2

#Missclasification error:
1-sum(diag(conf_matrix2))/sum(conf_matrix2)
#The missclasification error is 0.2722646

```


# 2.5 LDA
```{r}
library(MASS)
lda1 <- lda(train.tennis$y ~ x1 + x2 , data=train.tennis)
lda.predict1 = predict(lda1, test.tennis)
lda1
summary(lda1)

#Confusion matrix

lda.class=as.numeric(lda.predict1$class)-1
threshold=0.5
predicted_values3<-ifelse(lda.class>threshold,1,0)
actual_values3<-test.tennis$y
conf_matrix3<-table(predicted_values3,actual_values3)
conf_matrix3

#Missclasification Error formula: 
1-sum(diag(conf_matrix3))/sum(conf_matrix3)

#the missclassification error is 0.2360406
```

# 2.6 True Quality
```{r}
sum(lda.predict1$posterior[,1]>=.5)
sum(lda.predict1$posterior[,1]<.5)


sum(lda.predict1$posterior[,1]>.8) 
#67 matches can be predicted with a probability larger than 80%.

```

# Given Code: KNN
```{r}
ks = 1:30
yhat = sapply(ks, function(k){
class::knn(train=tn[tr,-1], cl=tn[tr,1], test=tn[,-1], k = k) #test both train and test
})
train.e = colMeans(tn[tr,1]!=yhat[tr,]) 
test.e = colMeans(tn[-tr,1]!=yhat[-tr,])

#optimal K choosen by  cross-validation: Here Training Data divided in 5 folds
library(caret)

set.seed(0)
ks = 1:30 # Choose K from 1 to 30.
idx = createFolds(tn[tr,1], k=5) # Divide the training data into 5 folds. # "Sapply" is a more efficient for-loop.
# We loop over each fold and each value in "ks"
# and compute error rates for each combination.
cv = sapply(ks, function(k){
sapply(seq_along(idx), function(j) {
yhat = class::knn(train=tn[tr[ -idx[[j]] ], -1],
cl=tn[tr[ -idx[[j]] ], 1],
test=tn[tr[ idx[[j]] ], -1], k = k) 
mean(tn[tr[ idx[[j]] ], 1] != yhat)
}) })

```
# 2.7 Matrix
```{r}
View(cv)
#Meaning of stored numbers: 
#The stored numbers in cv. are the result from sapply command. This command was used to get one vector out of the five folds. CV is a matrix for all 30 different K's and thus the stored numbers are the error rates for the associated K.

#Average
cv.e = colMeans(cv)
cv.e
#standard error of the average cv error over all five folds
cv.se= apply(cv, 2, sd)/sqrt(5)

#Smallest Se Error corresponding K
View(cv.se)
which.min(cv.se)
# K=25 has the lowest Error
```

# 2.8 Plot Missclassification
```{r}
library(colorspace)
co = rainbow_hcl(3)
par(mar=c(4,4,1,1)+.1, mgp = c(3, 1, 0))
plot(ks, cv.e, type="o", pch = 16, ylim = c(0, 0.7), col = co[2],
xlab = "Number of neighbors", ylab="Misclassification error")
arrows(ks, cv.e-cv.se, ks, cv.e+cv.se, angle=90, length=.03, code=3, col=co[2])
lines(ks, train.e, type="o", pch = 16, ylim = c(0.5, 0.7), col = co[3])
lines(ks, test.e, type="o", pch = 16, ylim = c(0.5, 0.7), col = co[1])
legend("topright", legend = c("Test", "5-fold CV", "Training"), lty = 1, col=co)
```
Answer: #The higher the number of neighbours the lower are the missclasication errors in all three models. The best perfomance has the 5-fold-CV with around 30 neighbors. Thus the bias in y(x) and variance get lower if K increases.

# 2.9
Comparison: Each Graph shown splitted under 2.9a-2.9c

If K=1 the decision boundary is highly flexible and thus has a low bias but a high variance. As K gets higher the plots show that there is less #flexibility in the models. The higher K gets the closer the model gets to a linear relationship. Thus this comes with low variance but a higher #bias. With a higher flexibility the test error rate declines.
If you set K=500 the boundary gets to a vertical line which is due to lost flexibility because of the high chosen value of K's.

```{r}
library(class)

k = 30
size = 100
xnew = apply(tn[tr,-1], 2, function(X) seq(min(X), max(X), length.out=size))
grid = expand.grid(xnew[,1], xnew[,2])
grid.yhat = knn(tn[tr,-1], tn[tr,1], k=k, test=grid)
np = 300
par(mar=rep(2,4), mgp = c(1, 1, 0))
contour(xnew[,1], xnew[,2], z = matrix(grid.yhat, size), levels=.5,
xlab=expression("x"[1]), ylab=expression("x"[2]), axes=FALSE,
main = paste0(k,"-nearest neighbors"), cex=1.2, labels="")
points(grid, pch=".", cex=1, col=grid.yhat)
points(tn[1:np,-1], col=factor(tn[1:np,1]), pch = 1, lwd = 1.5)
legend("topleft", c("Player 1 wins", "Player 2 wins"),
col=c("red", "black"), pch=1)
box()
```
Answer: The graph shows the KNN approach with K =30 with red and black observations, for player 1 and player 2. There are shown the KNN decision boundary als the black line. The red area shows in which test obervations will be assigned to player 1 and same for player 2. It shows all possible values for x1 and x2 and their decision boundary. 




##2.9a

```{r}
#K=1
k = 1
size = 100
xnew = apply(tn[tr,-1], 2, function(X) seq(min(X), max(X), length.out=size))
grid = expand.grid(xnew[,1], xnew[,2])
grid.yhat = knn(tn[tr,-1], tn[tr,1], k=k, test=grid)
np = 300
par(mar=rep(2,4), mgp = c(1, 1, 0))
contour(xnew[,1], xnew[,2], z = matrix(grid.yhat, size), levels=.5,
xlab=expression("x"[1]), ylab=expression("x"[2]), axes=FALSE,
main = paste0(k,"-nearest neighbors"), cex=1.2, labels="")
points(grid, pch=".", cex=1, col=grid.yhat)
points(tn[1:np,-1], col=factor(tn[1:np,1]), pch = 1, lwd = 1.5)
legend("topleft", c("Player 1 wins", "Player 2 wins"),
col=c("red", "black"), pch=1)
box()

```

##2.9b
```{r}
k = 50
size = 100
xnew = apply(tn[tr,-1], 2, function(X) seq(min(X), max(X), length.out=size))
grid = expand.grid(xnew[,1], xnew[,2])
grid.yhat = knn(tn[tr,-1], tn[tr,1], k=k, test=grid)
np = 300
par(mar=rep(2,4), mgp = c(1, 1, 0))
contour(xnew[,1], xnew[,2], z = matrix(grid.yhat, size), levels=.5,
xlab=expression("x"[1]), ylab=expression("x"[2]), axes=FALSE,
main = paste0(k,"-nearest neighbors"), cex=1.2, labels="")
points(grid, pch=".", cex=1, col=grid.yhat)
points(tn[1:np,-1], col=factor(tn[1:np,1]), pch = 1, lwd = 1.5)
legend("topleft", c("Player 1 wins", "Player 2 wins"),
col=c("red", "black"), pch=1)
box()

```
##2.9c
```{r}
#k=300
k = 300
size = 100
xnew = apply(tn[tr,-1], 2, function(X) seq(min(X), max(X), length.out=size))
grid = expand.grid(xnew[,1], xnew[,2])
grid.yhat = knn(tn[tr,-1], tn[tr,1], k=k, test=grid)
np = 300
par(mar=rep(2,4), mgp = c(1, 1, 0))
contour(xnew[,1], xnew[,2], z = matrix(grid.yhat, size), levels=.5,
xlab=expression("x"[1]), ylab=expression("x"[2]), axes=FALSE,
main = paste0(k,"-nearest neighbors"), cex=1.2, labels="")
points(grid, pch=".", cex=1, col=grid.yhat)
points(tn[1:np,-1], col=factor(tn[1:np,1]), pch = 1, lwd = 1.5)
legend("topleft", c("Player 1 wins", "Player 2 wins"),
col=c("red", "black"), pch=1)
box()
```

#=====================================================================================================================
##Task 3: Cross-Validation and Bootstrapping

```{r}
#install.packages("boot")
#install.packages("plyr")
#install.packages("ISLR")
#install.packages("leaps")

library(plyr)
library(ISLR)
data(Carseats)
```

#1. 
```{r}
lm.fit_cs <- lm(Sales ~ Price + Urban + US, data = Carseats)
print(lm.fit_cs)
summary(lm.fit_cs)$coef
```

#2a.
Split the sample set into a training set and a validation set, each encompassing half of the data. Use the random seed 2019.
```{r}
set.seed(2019)

# take 200 random row numbers

training_set_index <- sort(sample(nrow(Carseats), 200))

# use the 200 random rows for training set, rest for validation

training_set <- Carseats[training_set_index,]
validation_set <- Carseats[-training_set_index,]
```

#2b.
```{r}
lm.fit_cstr <- lm(Sales ~ Price + Urban + US, data=training_set)
print(lm.fit_cstr)
summary(lm.fit_cstr)$coef
summary(lm.fit_cs)$coef

summary(lm.fit_cs)$coef-summary(lm.fit_cstr)$coef
```
Answer: higher standard errors, lower p-values. Estimates a little different. Not as accurate. Influence of UrbanYes in training set higher, more significant and in other direction than with all data!

#2c.
```{r}
predicted <- predict(lm.fit_cstr, Carseats, type = "response")
# Evaluate error
predicted_val <- predict(lm.fit_cstr, validation_set, type = "response")
MSE <- mean((predicted_val-validation_set$Sales)^2)
# or
library(MLmetrics)
MSE.test <- MSE(y_pred = predicted_val, y_true = validation_set$Sales)
```

#2d.

#2018
```{r}
set.seed(2018)
training_set_index <- sort(sample(nrow(Carseats), 200))
training_set <- Carseats[training_set_index,]
validation_set <- Carseats[-training_set_index,]
lm.fit_cstr <- lm(Sales ~ Price + Urban + US, data=training_set)
predicted <- predict(lm.fit_cstr, Carseats, type = "response")
predicted_val <- predict(lm.fit_cstr, validation_set, type = "response")
MSE18 <- mean((predicted_val-validation_set$Sales)^2)

set.seed(2020)
training_set_index <- sort(sample(nrow(Carseats), 200))
training_set <- Carseats[training_set_index,]
validation_set <- Carseats[-training_set_index,]
lm.fit_cstr <- lm(Sales ~ Price + Urban + US, data=training_set)
predicted <- predict(lm.fit_cstr, Carseats, type = "response")
predicted_val <- predict(lm.fit_cstr, validation_set, type = "response")
MSE20 <- mean((predicted_val-validation_set$Sales)^2)

print(MSE)
print(MSE18)
print(MSE20)

```

#2e.
```{r}
#k = N, error is mean

library(boot)
Carseats.glm <- glm(Sales ~ Price + Urban + US, data = Carseats)
cv.err <- cv.glm(Carseats, Carseats.glm)$delta
cv.err.loo <- cv.glm(Carseats, Carseats.glm, K = nrow(Carseats))$delta

# As this is a linear model we could calculate the leave-one-out 
# cross-validation estimate without any extra model-fitting.

muhat <- fitted(Carseats.glm)
Carseats.diag <- glm.diag(Carseats.glm)
cv.err <- mean((Carseats.glm$y - muhat)^2/(1 - Carseats.diag$h)^2)
```

#3.
```{r}
library(leaps)

# delist ShelveLoc variable from dataset

Carseats[,"ShelveLoc"] <- list(NULL)
reg <- regsubsets(Sales ~ ., Carseats, nvmax = 3)
summary(reg)
# The best three variables are CompPrice, Advertising and Price.
# This indicates that they are the strongest predictors.

```

# multiple linear regression

```{r}

Carseats2.glm <- glm(Sales ~ Price + CompPrice + Advertising, data = Carseats)
cv.err2 <- cv.glm(Carseats, Carseats2.glm)$delta
cv.err2.loo <- cv.glm(Carseats, Carseats2.glm, K = nrow(Carseats))$delta
```

#4.
```{r}
# 4
mean(Carseats$Sales)
set.seed(1)
# sample matrix with 20 random samples (400 obs each with replacement)
sample <- matrix(,400,20) 
for(i in 1:20){
  sample[,i] <- sample(Carseats$Sales, replace = TRUE)
}

# enter mean command 20 times
mean(sample[,1])
mean(sample[,2])
mean(sample[,3])
mean(sample[,4])
mean(sample[,5])
mean(sample[,6])
mean(sample[,7])
mean(sample[,8])
mean(sample[,9])
mean(sample[,10])
mean(sample[,11])
mean(sample[,12])
mean(sample[,13])
mean(sample[,14])
mean(sample[,15])
mean(sample[,16])
mean(sample[,17])
mean(sample[,18])
mean(sample[,19])
mean(sample[,20])

# or use loop to create list with means of the 20 bootstrapped replicas

mean.list <- list()
for (i in 1:20){
  temp <- sample(Carseats$Sales, replace = TRUE)
  
  mean.list[i] <- mean(temp)
  rm(temp)
}
# range: lowest to highest mean
range(mean.list)
# 7.266800 to 7.703675
```

#5. 
```{r}
#Bootstrap 95% CI for R-Squared

sample_mean <- function(data, indices) {
  sample <- data[indices] # allows boot to select sample 
  bar <- mean(sample)
  return(bar)
} 
# bootstrapping with 20 replications 
set.seed(1)
results <- boot(data=Carseats$Sales, statistic=sample_mean, 
                R=20)
# view results
results
plot(results)
# get 95% confidence interval
# Bootstrap-CI does not work with so few replications -> choose R=400
set.seed(1)
results <- boot(data=Carseats$Sales, statistic=sample_mean, R=400)
results
plot(results)
boot.ci(results, conf=0.95)
```

#=====================================================================================================================
##Task 4: Linear Model Selection and Regularization

```{r}
set.seed(1007)
diamonds <- diamonds
sm <- sample.int(nrow(diamonds), nrow(diamonds)/10)
smbi <- rep(1, nrow(diamonds))
smbi[sm]  <- 0
smbi[sm]
diamonds2  = data.frame(diamonds, "issmall" = as.factor(smbi) )
small = (diamonds2$issmall == 0)
diamondssm = diamonds2[small, ]
```

#1.

Three highest prices
```{r}
Highprice <- diamondssm$price[order(-diamondssm$price)[1:3]]
Highprice 
# The 3 highest prices are 18795, 18779 and 18759
```

How many carats
```{r}
expensive <- subset(diamondssm, diamondssm$price >= Highprice[3])
print(expensive$carat) 

# The carats weight 2.00, 2.06 and 2.00
```

Mean weight
```{r}
# of 3 highest prices:

mean(expensive$carat) 
# 2.02

# of all diamonds:

mean(diamondssm$carat) 
# 0.8002299
```

Which colour is the most prevalent?
```{r}
names(which.max(table(diamondssm$color))) 

# most prevalent colour equals "G"
```

Plots
```{r}
plot(diamondssm$carat, diamondssm$price, col = "red", main  = "Relationship between carat and price ")
plot(log(diamondssm$carat), log(diamondssm$price), col = "blue", main  = "Relationship between log(carat) and log(price) ")
```

Given:
```{r}
diamondssm <- data.frame(diamondssm, "logprice" = log(diamondssm$price), "logcarat" = log(diamondssm$carat))
#diamonds3 <-  subset(diamondssm, select = -c(price, carat, issmall))
diamonds3 <-  subset(diamondssm, select = c(cut, color, clarity, depth,table, x,y,z, logprice, logcarat ))
```

#2a.
forward stepwise selection
```{r}
# check for missingsness of values
is.na(diamonds3) 
sum(is.na(diamonds3))  # 0 missing values
diamonds3 <- na.omit(diamonds3) #  remove N/A

```

#2b.
backward selection model
```{r}
#forward selection 

fwd.model <- regsubsets(logprice~., data = diamonds3, method = "forward")
fwd.summary <- summary(fwd.model)
fwd.summary
names(fwd.summary)

# How large is the best subset from the forward stepwise selection? 
fwd.summary$rsq <- as.vector(fwd.summary$rsq )

fwd.summary$rsq
# Search for largest Rsq?
B <- cbind(c(1:8), fwd.summary$rsq)

max(fwd.summary$rsq) # 0.9825262 which corresponds to the 8th row of A and which includes cut.L, colorL, colorQ, ClarityL,Q,C,$ and Carat

#backward selection

bwd.model <- regsubsets(logprice~., data = diamonds3, method = "backward")
bwd.summary <- summary(bwd.model)
names(bwd.summary)

# How large is the best subset from the backward stepwise selection? 
bwd.summary$rsq <- as.vector(bwd.summary$rsq )

A <- cbind(c(1:8), bwd.summary$rsq)
# Search for largest Rsq?
max(bwd.summary$rsq)  
#rsq = 0.9825262 which corresponds to the 8th row of A and which includes cut.L, colorL, colorQ, ClarityL,Q,C,$ and Carat
```

#3.

R-Squared indicates  how well the data fits the model. It takes values from 0 to 1. If R-Squared equals 0.8, 80 percent of the data can be explained by the model. Usually, a high R-Squared means a good fitting model. But sometimes a high RSquared results in an overfitted and therefore biased model. Since it is dependent on the number of predictors, ist value increases if more predictors are included. TTherefore R-Squared is not a good measure for choosing one model over the others, given the fact that the number of predictors differs in the models. 

MSE interpreted as the expected square distance that an estimator has from the true value. Compared tot he R-Squared ??? which is better the higher value it takes ??? we have to choose the MSE with the lowest value. Furthermore MSE indcates whether a model is overfitted, which is a problem using Rsquared.

#4.

Ridge regression
```{r}

x = model.matrix(logprice~.,diamonds3)[,-9]
y = diamonds3$logprice

 grid =10^ seq (10,-2, length =100)
 ridge.mod =glmnet (x,y,alpha =0, lambda =grid)
 
 dim(coef(ridge.mod )) 
# 24 rows(one for each predictor, plus intercept) 
# and 100 columns(one for each value of lambda)
 
 ridge.mod$lambda[50] 
 
# Lambda equals 1149,57
 
 coef(ridge.mod)[,50]
 
 

# split into training and test set
 
 set.seed(23)
 train = sample(1:nrow(x), nrow(x)/2)
 test = (-train)
 y.test = y[test]

# fit ridge regression
 
ridge.mod =glmnet (x[train,],y[train],alpha =0, lambda =grid )
ridge.pred=predict (ridge.mod ,s=4, newx=x[test ,])
 
mean(( ridge.pred -y.test)^2) 
# MSE equals 0.314796
```

Cross-Validation
```{r}
set.seed(23)
cv.out <- cv.glmnet(x[train,], y[train], alpha = 0)
plot(cv.out)
bestfitlam <- cv.out$lambda.min
bestfitlam  # best fitting lambda equals 0.09756284

# Mean Squared error associated with smallest cross validation error of 0.09756284, set s =  best fitting lambda chosen from ross validation
# predictions for test set

best.ridge <- predict(ridge.mod, s = bestfitlam, newx = x[test,])
mean((best.ridge-y.test)^2) 
# mean squared error equals 0.02998839 which is smaller than f.e. an lambda of 4 [MSE = 0,314796]


#refit ridge regression using optimal lambda chosen by cross-validation

out  <- glmnet(x,y, alpha = 0)
predict(out, type = "coefficients", s = bestfitlam)[1:24,] # since number of independent variables fits 24

ridge.model <- predict(out, type = "coefficients", s = bestfitlam)[1:24,]
ridge.model
``` 

Lasso-model

difference to ridge regression: lasso regression assumes alpha to equal one instead of zero
```{r}
lasso.mod <- glmnet(x[train,], y[train], alpha = 1)
plot(lasso.mod)


set.seed (23)
cv.out.lasso  <- cv.glmnet (x[train ,],y[train],alpha =1) # detect smallest cross validation error 
plot(cv.out.lasso)
bestfitlam  <- cv.out$lambda.min # choose lambda with smallest cross validation error 
lasso.pred <- predict (lasso.mod ,s=bestfitlam ,newx=x[test ,]) # predict 
lasso.pred
mean(( lasso.pred -y.test)^2) 

# Lasso-MSE equals 0.06837037

out <- glmnet (x,y,alpha =1, lambda =grid)
lasso.coef <- predict (out ,type ="coefficients",s=bestfitlam )[1:24 ,] # predict lasso regression on log price
lasso.coef # lasso regression shows some coefficient estimates which equal zero


lasso.coef[lasso.coef !=0] # show all coefficients estimates which do not equal zero

# lasso model with lambda chosen by cross-validation contains only 4 variables, clarity, width and carat
```

Simple Regression
```{r}
lin.mod <- lm(y~diamonds3$cut + diamonds3$color + diamonds3$clarity + diamonds3$depth +  diamonds3$table + diamonds3$x + diamonds3$y +  diamonds3$z + diamonds3$logcarat )
summary(lin.mod)
```

Comparison:

Ridge regression and multiple linear model:
Estimates of both models show similar results. Both have a positive intercept and signs of almost all predictors have the same direction. Only predictor z (depth) has , considering the linear model, a negative effect on the dependent variable whereas, considering the ridge regression, depth has a positive impact on a diamond??s price. This might be due tot he fact that depth is calculated from x,y and z, so in the linear model there might be a bias due to multicollinearity. But since variable z is only significant at a very low level, we might not consider it as a expressive predictor anyway. 

lasso regression and multiple linear model:
The lasso model performs variable selection, whereas the multiple linear model shows the relationship and significance between all included independent variables and y.Apart from the fact that the lasso regression only considers three independent variables [clarity.L, y and logcarat] and the intercept, whereas the multiple linear model estimates 16 (high) significant predictors, the range of the predictors are quite similar. Both intercepts have an effect of about 7, whereas the ridge regression measured an predictor about 3.64. The lasso regression only considers those dependent variables which had a high impact and a high significance level in the multiple linear model as well, 
Ridge regression and lasso regression:
As written above, lasso regression only considers 3 independent variables. Therefore it is difficult to compare both models. But both have a positive intercept and signs of all predictors have the same direction. However, it might be neccessary that, in a direct comparison of those predictors considered in both models, the lasso regression??s magnitude is higher than in the ridge model. This might be due tot he fact that there exists collinearity in the ridge model which is eliminated in the lasso regression. 

```{r}
ridge.model
lasso.coef
summary(lin.mod)

```

#=====================================================================================================================
